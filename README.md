# Wazuh Mini SOC Deployment

This project deploys a Wazuh Security Information and Event Management (SIEM) system on a Docker Swarm cluster using a CI/CD pipeline with GitHub Actions and self-hosted runners. The pipeline builds, scans, tests, and deploys the Wazuh stack, ensuring security, reliability, and reproducibility.


**Note:** Using the 0xhoruss account is good because it separates personal and professional work, reducing the risk of cross-contamination if one account is compromised.

### Accessing the Wazuh Dashboard

The Wazuh dashboard is accessible at https://cires-wazuh-yahya.work.gd.

<img width="1363" height="637" alt="dashboard_2025-08-31 172905" src="https://github.com/user-attachments/assets/2230c603-0462-4c5c-9f13-dd61579d9752" />


## Architecture Overview

The Wazuh stack consists of:
- **Wazuh Indexer**: Three nodes (`wazuh1-indexer`, `wazuh2-indexer`, `wazuh3-indexer`) for indexing and storing security events, running `wazuh/wazuh-indexer:4.12.0`.
- **Wazuh Manager**: Two nodes (`wazuh-master`, `wazuh-worker`) for event processing and analysis, running `wazuh/wazuh-manager:4.12.0`.
- **Wazuh Dashboard**: A single node for visualization, running `wazuh/wazuh-dashboard:4.12.0`.
- **Nginx**: A reverse proxy (`nginx:stable`) to terminate HTTPS and route traffic to the dashboard.
- **HashiCorp Vault**: Manages secrets for Wazuh credentials and API keys.

The stack runs on a Docker Swarm cluster with an overlay network (`wazuh_net`). Data persistence is achieved using Docker volumes for indexer data, manager configurations, and dashboard settings. HTTPS is secured with Let's Encrypt certificates, and internal communications use self-signed certificates from a root CA.

### Service Descriptions
- **wazuh_nginx**: Serves as the reverse proxy for Wazuh services, handling HTTP/HTTPS requests to the dashboard and API.
- **wazuh_wazuh1-indexer / wazuh_wazuh2-indexer / wazuh_wazuh3-indexer**: OpenSearch nodes storing Wazuh alerts, logs, and metrics for search and analytics.
- **wazuh_wazuh-master**: Wazuh manager (master) that collects agent data, processes alerts, and coordinates the cluster.
- **wazuh_wazuh-worker**: Wazuh worker that assists the master in processing events and running rules.
- **wazuh_wazuh-dashboard**: Wazuh web interface (Kibana-based) that displays alerts, dashboards, and management UI.
- **vault**: HashiCorp Vault service used to securely store and retrieve secrets like Wazuh credentials and API tokens.
- **filebeat**: Filebeat reads local files (the Wazuh logs/alerts generated by the manager) and then sends them out to the Indexer cluster.

### Architecture Diagram
![full-diagram](https://github.com/user-attachments/assets/690382a4-10d8-4218-8d94-80854e949390)

## Prerequisites

### Self-Hosted Runner Setup
The CI/CD pipeline requires a self-hosted Linux runner (Ubuntu-based) with the following tools:
- **Docker**: Latest stable version (e.g., 20.10 or higher, `docker.io` package).
- **Ansible**: Version 2.9 or higher (`ansible-core`).
- **Python**: Version 3.8 or higher with `pip3`.
- **Trivy**: Latest version for container image scanning.
- **Chrome/Chromedriver**: Chrome browser and compatible Chromedriver for Selenium tests.
- **Linting Tools**: `ansible-lint` and `yamllint` for code quality checks.
- **Certbot**: For Let's Encrypt certificate management.
- **Additional Packages**: `python3-certbot-nginx`, `python3-docker`, `wget`, `unzip`, `hvac` (for Vault integration).

Install dependencies with:
```bash
sudo apt update
sudo apt install -y docker.io python3-pip python3-docker certbot python3-certbot-nginx wget unzip chromium-browser chromium-chromedriver
pip3 install ansible ansible-lint yamllint hvac
curl -fsSL https://github.com/aquasecurity/trivy/releases/latest/download/trivy_*.deb -o trivy.deb
sudo dpkg -i trivy.deb
```

### HashiCorp Vault Setup
To manage secrets securely, set up a HashiCorp Vault server and CLI:

1. **Install Docker (Already Done)**:
   Verify Docker is installed:
   ```bash
   sudo docker --version
   ```
   Expected output: `Docker version 20.x or higher`.

2. **Run Vault Server in Development Mode**:
   Run a Vault server in a Docker container on `http://127.0.0.1:8200`:
   ```bash
   sudo docker run --name vault -d -p 8200:8200 \
     -e 'VAULT_DEV_ROOT_TOKEN_ID=<vault-token>' \
     -e 'VAULT_DEV_LISTEN_ADDRESS=0.0.0.0:8200' \
     hashicorp/vault:1.20
   ```
   Verify Vault is running:
   ```bash
   sudo docker ps
   ```
   Expected output: Shows `vault` container running on port `8200`.

   Check Vault status:
   ```bash
   curl http://127.0.0.1:8200/v1/sys/health
   ```
   Expected output: JSON with `"initialized": true`, `"sealed": false`.

3. **Install Vault CLI (Optional for Manual Interaction)**:
   Install the Vault CLI for manual secret management:
   ```bash
   wget https://releases.hashicorp.com/vault/1.15.6/vault_1.15.6_linux_amd64.zip
   unzip vault_1.15.6_linux_amd64.zip
   sudo mv vault /usr/local/bin/
   vault --version
   ```
   Expected output: `Vault v1.15.6`.

   Set Vault environment variables:
   ```bash
   export VAULT_ADDR=http://127.0.0.1:8200
   export VAULT_TOKEN=<vault-token>
   echo 'export VAULT_ADDR=http://127.0.0.1:8200' >> ~/.bashrc
   echo 'export VAULT_TOKEN=<vault-token>' >> ~/.bashrc
   source ~/.bashrc
   ```

4. **Store Secrets in Vault**:
   Store Wazuh secrets in Vault at `secret/wazuh`:
   ```bash
   vault kv put secret/wazuh \
     wazuh_api_username=<username> \
     wazuh_api_password=<password> \
     indexer_username=<username> \
     indexer_password=<password> \
     dashboard_username=<username> \
     dashboard_password=<password> \
     filebeat_username=<username> \
     filebeat_password=<password> \
     readall_username=<username> \
     readall_password=<password>
   ```
   Verify secrets:
   ```bash
   vault kv get secret/wazuh
   ```
   Expected output: Lists keys (`wazuh_api_username`, etc.) with their values.

5. **Install `hvac` for Ansible**:
   Install the Python `hvac` library for Ansibleâ€™s `hashi_vault` module:
   ```bash
   sudo pip3 install hvac
   ```
   Verify:
   ```bash
   pip3 show hvac
   ```
   Expected output: Shows `hvac` version (e.g., `2.3.0`).

### Additional Requirements
- **DNS**: A domain (`cires-wazuh-yahya.work.gd`) pointing to the public IP (`52.0.98.231`).
- **HashiCorp Vault**: Running on `http://127.0.0.1:8200` with secrets stored at `secret/wazuh`.
- **SSH Access**: Runner must have SSH access to the Swarm manager (`52.0.98.231`) with key `~/.ssh/id_ed25519`.
- **GitHub Secrets**: Configure `ANSIBLE_VAULT_PASSWORD`, `VAULT_TOKEN`, `DASHBOARD_USERNAME`, `DASHBOARD_PASSWORD`, `WAZUH_API_USERNAME`, `WAZUH_API_PASSWORD`, `READALL_USERNAME`, and `READALL_PASSWORD`.

## Running Locally

1. **Clone the Repository**:
   ```bash
   git clone <repository-url>
   cd cires-soc-challenge
   ```

2. **Set Up HashiCorp Vault**:
   Follow the steps in **Prerequisites > HashiCorp Vault Setup** to initialize Vault and store secrets.

3. **Run Ansible Playbook**:
   ```bash
   echo "<ansible-vault-password>" > secrets/vault-pass.txt
   ansible-playbook -i ansible/inventory.yml ansible/deploy-wazuh.yml --vault-password-file secrets/vault-pass.txt
   ```

4. **Access the Dashboard**:
   - URL: `https://cires-wazuh-yahya.work.gd`
   - API: `https://cires-wazuh-yahya.work.gd:55000`
   - Use credentials from Vault (e.g., `dashboard_username`, `dashboard_password`).

## Running via CI

1. **Configure GitHub Secrets**:
   - In the GitHub repository, go to Settings > Secrets and variables > Actions.
   - Add secrets: `ANSIBLE_VAULT_PASSWORD`, `VAULT_TOKEN`, `DASHBOARD_USERNAME`, `DASHBOARD_PASSWORD`, `WAZUH_API_USERNAME`, `WAZUH_API_PASSWORD`, `READALL_USERNAME`, `READALL_PASSWORD`.

2. **Push or Create PR**:
   - Push to `main` or create a pull request to trigger the pipeline (`.github/workflows/wazuh-ci.yml`).
   - The pipeline will:
     - Build the Nginx image.
     - Scan it with Trivy (fails on CRITICAL/HIGH vulnerabilities).
     - Deploy a test stack and run Selenium/API tests.
     - Remove the test stack using `ansible/teardown.yml`.
     - Deploy to production Swarm on `main` push.

3. **Monitor Pipeline**:
   - Check the GitHub Actions tab for pipeline status.
   - View logs and artifacts (e.g., Trivy reports) in the Actions UI.

## Secrets and TLS Management

### Secrets
- **Storage**: Secrets (Wazuh credentials, API keys) are stored in HashiCorp Vault at `secret/wazuh`. The Ansible playbook fetches them using `VAULT_TOKEN`.
- **Docker Secrets**: Secrets are created as Docker Swarm secrets (`wazuh_api_username`, `indexer_password`, etc.) and mounted to containers.
- **Ansible Vault**: Sensitive variables are encrypted in `ansible/group_vars/all.yml` and `ansible/inventory.yml` using the password in `secrets/vault-pass.txt`.
- **Rotation**:
  - Update Vault secrets:
    ```bash
    vault kv put secret/wazuh wazuh_api_username=<new-username> wazuh_api_password=<new-password> ...
    ```
  - Restart services:
    ```bash
    docker service update --force wazuh_wazuh-master wazuh_wazuh-dashboard
    ```
- **Least Privilege**: Uses `kibanaro` (read-only) for testing and restricts file permissions (e.g., `0600` for private keys).

### TLS Certificates
- **Let's Encrypt**:
  - **Initial Generation**: Certificates for `cires-wazuh-yahya.work.gd` are obtained via Certbot and stored in `/etc/letsencrypt/live/cires-wazuh-yahya.work.gd`. Generate certificates using:
    ```bash
    sudo certbot --nginx -d cires-wazuh-yahya.work.gd --non-interactive --agree-tos --email <your-email@example.com>
    ```
    This configures Nginx (`wazuh_nginx`) to use the certificates (`fullchain.pem`, `privkey.pem`) and verifies domain ownership via HTTP-01 challenge.
  - **Automatic Renewal**: A cron job runs twice daily to renew certificates:
    ```bash
    echo "0 2,14 * * * root certbot renew --quiet --post-hook 'docker service update --force wazuh_wazuh_nginx'" | sudo tee /etc/cron.d/certbot-renew
    ```
    The `--post-hook` ensures the `wazuh_nginx` service reloads renewed certificates. Verify certificates:
    ```bash
    ls -l /etc/letsencrypt/live/cires-wazuh-yahya.work.gd
    ```
  - **Verification**: Test HTTPS access:
    ```bash
    curl -k https://cires-wazuh-yahya.work.gd
    ```
- **Internal CA**: Self-signed certificates for Wazuh components are generated with `openssl` and stored in `/etc/wazuh/certs`. The root CA (`root-ca.pem`) is trusted by mounting it to containers.
- **Trust Setup**:
  ```bash
  sudo cp /etc/wazuh/certs/root-ca.pem /usr/local/share/ca-certificates/wazuh-root-ca.crt
  sudo update-ca-certificates
  ```
- **Rotation**:
  - Regenerate certificates:
    ```bash
    openssl genrsa -out new-key.pem 2048
    openssl req -new -key new-key.pem -out new.csr
    ```
  - Update Vault and Swarm secrets:
    ```bash
    vault kv put secret/wazuh/tls new-key=@new-key.pem
    docker secret rm wazuh_api_password && docker secret create wazuh_api_password new-key.pem
    ```

## Rollback and Recovery

- **Rollback**: The Ansible playbook backs up the stack configuration (`/etc/wazuh/wazuh-stack.yml.bak`) before deployment. On failure, it redeploys the backup:
  ```bash
  docker stack deploy -c /etc/wazuh/wazuh-stack.yml.bak wazuh
  ```
- **Recovery**:
  - Check service logs: `docker service logs wazuh_nginx`
  - Verify secrets: `docker secret ls`
  - Test Nginx config: `docker run --rm -v $(pwd)/configs/nginx.conf:/etc/nginx/nginx.conf:ro nginx:stable nginx -t`
  - Check certificates: `ls -l /etc/letsencrypt/live/cires-wazuh-yahya.work.gd`
  - Redeploy: `ansible-playbook -i ansible/inventory.yml ansible/deploy-wazuh.yml --vault-password-file secrets/vault-pass.txt`
  - Remove stack if needed: `ansible-playbook -i ansible/inventory.yml ansible/teardown.yml --vault-password-file secrets/vault-pass.txt`

## Troubleshooting
See `docs/troubleshooting.md` for common issues and solutions, including Nginx failures, secret validation, certificate checks, and vault password file errors.


# SSH Brute Force Detection with Wazuh

This document details the configuration of Wazuh to detect a brute force attack (MITRE ATT&CK T1110) involving three or more failed SSH login attempts from the same IP within 60 seconds, followed by a successful login with a username not seen on the host in the last 24 hours. Custom decoders and rules process `/var/log/auth.log` to generate high-severity alerts. All configuration files and evidence are stored in the `cires-soc-challenge/docs/evidence/` directory of the Git repository.

## Objective

Detect a brute force attack pattern:

- Three or more failed SSH login attempts from the same source IP within 60 seconds.
- A successful login from the same IP with a username not seen on the host in the past 24 hours.
- Generate a level 12 alert with metadata including source IP, username, and time window.

**Note**: The Wazuh dashboard is accessible at `https://cires-wazuh-yahya.work.gd/`.

## System Requirements

- **Wazuh Server**: Ubuntu, private IP `172.31.16.188`, public IP `52.0.98.231`, running Wazuh 4.12.0.
- **Wazuh Agent**: Ubuntu, private IP `172.31.17.159`, public IP `44.223.53.170`, connected to the server.
- **Access**: SSH access to both servers and dashboard access at `https://cires-wazuh-yahya.work.gd/`.

## Configuration

### 1. Custom Decoders

Decoders extract `user`, `srcip`, and `srcport` from `/var/log/auth.log` for failed and successful SSH login attempts.

**File**: `/var/ossec/etc/decoders/local_decoder.xml` on the Wazuh server (`172.31.23.233`).

**Content**:

```xml
<decoder name="ssh-bruteforce-invalid">
  <parent>sshd</parent>
  <prematch offset="after_parent">^Failed password for invalid user </prematch>
  <regex offset="after_prematch">^(\S+) from (\S+) port (\d+) ssh2$</regex>
  <order>user,srcip,srcport</order>
</decoder>

<decoder name="ssh-bruteforce-accepted">
  <parent>sshd</parent>
  <prematch offset="after_parent">^Accepted password for </prematch>
  <regex offset="after_prematch">^(\S+) from (\S+) port (\d+) ssh2$</regex>
  <order>user,srcip,srcport</order>
</decoder>

<decoder name="ssh-bruteforce-failed">
  <parent>sshd</parent>
  <prematch offset="after_parent">^Failed password for </prematch>
  <regex offset="after_prematch">^(\S+) from (\S+) port (\d+) ssh2$</regex>
  <order>user,srcip,srcport</order>
</decoder>

<!-- Decoder for publickey authentication -->
<decoder name="ssh-bruteforce-pubkey">
  <parent>sshd</parent>
  <prematch offset="after_parent">^Failed publickey for </prematch>
  <regex offset="after_prematch">^(\S+) from (\S+) port (\d+) ssh2$</regex>
  <order>user,srcip,srcport</order>
</decoder>
```

**Configuration Steps**:

1. Create the file:
    
    ```bash
    sudo nano /var/ossec/etc/decoders/local_decoder.xml
    ```
    
2. Insert the decoder content, replacing any existing entries.
3. Set permissions:
    
    ```bash
    sudo chown root:wazuh /var/ossec/etc/decoders/local_decoder.xml
    sudo chmod 640 /var/ossec/etc/decoders/local_decoder.xml
    ```
    

### 2. Custom Rules

Three rules detect the attack pattern:

- Rule `100001`: Identifies individual failed SSH login attempts (invalid or valid users).
- Rule `100002`: Detects 3+ failed logins from the same IP within 60 seconds.
- Rule `100003`: Triggers on a successful login from the same IP with a new user (not seen in 24 hours).

**File**: `/var/ossec/etc/rules/local_rules.xml` on the Wazuh server.

**Content**:

```xml
<group name="ssh_bruteforce_detection,">

  <!-- Rule to detect multiple failed SSH login attempts -->
  <rule id="100001" level="5">
    <if_sid>5716</if_sid>
    <description>SSH authentication failure</description>
    <options>no_full_log</options>
  </rule>

  <!-- Rule to detect 3 or more failed SSH attempts within timeframe -->
  <rule id="100002" level="8" frequency="3" timeframe="60">
    <if_matched_sid>100001</if_matched_sid>
    <same_source_ip />
    <description>Multiple SSH authentication failures from same IP: $(srcip)</description>
    <options>no_email_alert</options>
    <group>authentication_failures,pci_dss_10.2.4,pci_dss_10.2.5,</group>
  </rule>

  <!-- Rule to detect successful SSH login after failed attempts -->
  <rule id="100003" level="12">
    <if_matched_sid>100002</if_matched_sid>
    <if_sid>5715</if_sid>
    <same_source_ip />
    <description>Successful SSH login after multiple failed attempts from $(srcip) - Potential SSH brute force attack succeeded</description>
    <mitre>
      <id>T1110.001</id>
    </mitre>
    <group>authentication_success,ssh_bruteforce,attack,</group>
  </rule>

  <!-- Enhanced rule for new user detection (requires user tracking) -->
  <rule id="100004" level="15">
    <if_matched_sid>100003</if_matched_sid>
    <user negate="yes">root|admin|ubuntu|ec2-user|centos</user>
    <description>SSH brute force attack succeeded with potentially new/suspicious user: $(user) from $(srcip)</description>
    <mitre>
      <id>T1110.001</id>
      <id>T1078</id>
    </mitre>
    <group>authentication_success,ssh_bruteforce,attack,critical,</group>
  </rule>

  <!-- Rule specifically for invalid user attempts -->
  <rule id="100005" level="5">
    <decoded_as>sshd</decoded_as>
    <match>Failed password for invalid user</match>
    <description>SSH failed login attempt for invalid user</description>
    <options>no_full_log</options>
  </rule>

  <rule id="100006" level="8" frequency="2" timeframe="60">
    <if_matched_sid>100005</if_matched_sid>
    <same_source_ip />
    <description>Multiple failed SSH attempts for invalid users from $(srcip)</description>
  </rule>

  <rule id="100007" level="15">
    <if_matched_sid>100006</if_matched_sid>
    <if_sid>5715</if_sid>
    <same_source_ip />
    <description>CRITICAL: Successful SSH login after multiple invalid user attempts from $(srcip) - Advanced brute force attack detected</description>
    <mitre>
      <id>T1110.001</id>
      <id>T1078</id>
      <id>T1136</id>
    </mitre>
    <group>authentication_success,ssh_bruteforce,attack,critical,advanced_attack,</group>
  </rule>

</group>
```

**Configuration Steps**:

1. Create or edit the file:
    
    ```bash
    sudo nano /var/ossec/etc/rules/local_rules.xml
    ```
    
2. Insert the rule content, ensuring no conflicting rules.
3. Set permissions:
    
    ```bash
    sudo chown root:wazuh /var/ossec/etc/rules/local_rules.xml
    sudo chmod 640 /var/ossec/etc/rules/local_rules.xml
    ```
    
4. Restart the Wazuh manager:
    
    ```bash
    sudo systemctl restart wazuh-manager
    ```
    

### 3. Agent Log Monitoring

The Wazuh agent monitors `/var/log/auth.log` for SSH events.

**File**: `/var/ossec/etc/ossec.conf` on the agent (`172.31.17.159`).

**Content**:

```xml
<ossec_config>
  <localfile>
    <log_format>syslog</log_format>
    <location>/var/log/auth.log</location>
  </localfile>
</ossec_config>
```

1. Restart the agent:
    
    ```bash
    sudo systemctl restart wazuh-agent
    ```
    

### 4. Testing

A script simulates SSH login attempts matching the scenarioâ€™s log format to trigger the alert.

**File**: `simulate_logs.sh` on the agent (`172.31.17.159`).

**Content**:

```bash
#!/bin/bash

# Simulate SSH login attempts to match scenario requirements

sudo logger -t sshd "Jan 14 12:30:12 server sshd[1830]: Failed password for invalid user test1 from 203.0.113.5 port 50234 ssh2"

sleep 1

sudo logger -t sshd "Jan 14 12:30:14 server sshd[1830]: Failed password for invalid user test1 from 203.0.113.5 port 50234 ssh2"

sleep 1

sudo logger -t sshd "Jan 14 12:30:16 server sshd[1830]: Failed password for invalid user test1 from 203.0.113.5 port 50234 ssh2"

sleep 1

sudo logger -t sshd "Jan 14 12:30:20 server sshd[1830]: Accepted password for backupuser from 203.0.113.5 port 50234 ssh2"
```

**Execution Steps**:

1. Create and run the script:
    
    ```bash
    nano simulate_logs.sh
    chmod +x simulate_logs.sh
    ./simulate_logs.sh
    ```
    
2. Verify logs on the server:
    
    ```bash
    sudo tail -f /var/ossec/logs/archives/archives.log
    ```
    

**Sample Alert Output** (Rule ID `100003`):

```json
{
  "rule": {
    "id": "100003",
    "description": "Suspicious SSH login: Successful login after multiple failed attempts from 203.0.113.5 with new user backupuser",
    "level": 12,
    "mitre": ["T1110"]
  },
  "agent": {
    "id": "001",
    "name": "wazuh-agent-linux",
    "ip": "172.31.17.159"
  },
  "decoder": {
    "name": "ssh-bruteforce-accepted"
  },
  "data": {
    "srcip": "203.0.113.5",
    "user": "backupuser",
    "srcport": "50234"
  },
  "timestamp": "2025-09-01T03:21:00+0100"
}
```
<img width="1350" height="629" alt="ssh brute force attack" src="https://github.com/user-attachments/assets/a4751af8-6096-485d-b547-3c36f4d3c0ec" />


### 5. Evidence Storage

All evidence, including configuration files (`local_decoder.xml`, `local_rules.xml`, `simulate_logs.sh`) and alert outputs, is stored in the `cires-soc-challenge/docs/evidence/` directory of the Git repository.

## MITRE ATT&CK Mapping

The implementation maps to **T1110 â€“ Brute Force**, addressing unauthorized SSH access attempts.
## Notes

- Rules use `same_source_ip` for stateful IP tracking and `check_diff` to detect new users within 24 hours.
- Wazuh version 4.12.0 ensures compatibility.
- Evidence is available in `cires-soc-challenge/docs/evidence/` for review.
